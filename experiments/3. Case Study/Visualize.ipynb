{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, set_seed, pipeline\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "import networkx as nx\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from networkx.algorithms.dag import descendants, ancestors\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../modules'))\n",
    "from experiment_1.RoBERTaEntity import RoBERTaEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"__NE_FROM__\", \"__NE_TO__\", \"__NE_OTHER__\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"mining_processed.json\", convert_dates=[\"date\"])\n",
    "df = df[df['date'] >= datetime(2021, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767dc5edec2f4673bb4790e9af122e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/85971 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'date', 'link', 'sentence', 'from', 'to', 'masked_sentence'],\n",
       "    num_rows: 85971\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "def tokenize(examples):\n",
    "    tokens = tokenizer(examples['masked_sentence'], truncation=False, padding='max_length')\n",
    "    if len(tokens[\"input_ids\"]) <= 512:\n",
    "        return examples\n",
    "    else:\n",
    "        print(examples['masked_sentence'])\n",
    "ds = ds.filter(tokenize, batched=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'RoBERTaEntity' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = RoBERTaEntity.from_pretrained(\"../../CaseStudyModel\", local_files_only=True, num_labels=5) #, id2label=id2label, label2id=label2id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(\"cuda\")\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.pt_utils.KeyDataset at 0x259e98cb790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KeyDataset(ds, \"masked_sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c216286616a84cf8b3bbc23b52cda981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = []\n",
    "for out in tqdm(pipe(KeyDataset(ds, \"masked_sentence\"), batch_size=512), total=len(ds)):\n",
    "    if out['score'] >= 0.8:\n",
    "        labels.append(out['label'])\n",
    "    else:\n",
    "        labels.append(None)\n",
    "    #labels.extend([x['label'] for x in out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_result = defaultdict(lambda: [])\n",
    "for i, row in df.iterrows():\n",
    "    if row['from'] == \"Tesla Inc\":\n",
    "        row['from'] = \"Tesla\"\n",
    "    if row['to'] == \"Tesla Inc\":\n",
    "        row['to'] = \"Tesla\"\n",
    "    if 'Contemporary Amperex Technology' in row['from']:\n",
    "        row['from'] = \"CATL\"\n",
    "    if 'Contemporary Amperex Technology' in row['to']:\n",
    "        row['to'] = \"CATL\"\n",
    "    if row['from'] == row['to']:\n",
    "        continue\n",
    "    if row['result'] is None:\n",
    "        continue\n",
    "    if row['result'] not in ['B_supplies_A', 'A_supplies_B']:\n",
    "       continue\n",
    "    swap = row['from'] > row['to']\n",
    "    if swap:\n",
    "        if row['result'] == 'B_supplies_A':\n",
    "            scm_result[(row['to'], row['from'])].append('A_supplies_B')\n",
    "        else:\n",
    "            scm_result[(row['to'], row['from'])].append('B_supplies_A')\n",
    "    else:\n",
    "        if row['result'] == 'B_supplies_A':\n",
    "            scm_result[(row['from'], row['to'])].append('B_supplies_A')\n",
    "        else:\n",
    "            scm_result[(row['from'], row['to'])].append('A_supplies_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_buyer_result = []\n",
    "for key, value in scm_result.items():\n",
    "    if 'A_supplies_B' in set(value):\n",
    "        supplier_buyer_result.append(key)\n",
    "    elif 'B_supplies_A' in set(value):\n",
    "        supplier_buyer_result.append((key[1], key[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph(supplier_buyer_result)\n",
    "largest_strongly_connected_component = max(nx.strongly_connected_components(G), key=len)\n",
    "largest_strong_subgraph = G.subgraph(largest_strongly_connected_component)\n",
    "largest_weakly_connected_component = max(nx.weakly_connected_components(G), key=len)\n",
    "largest_weak_subgraph = G.subgraph(largest_weakly_connected_component)\n",
    "size_strong = len(largest_strong_subgraph.nodes())\n",
    "size_weak = len(largest_weak_subgraph.nodes())\n",
    "graphml_file_path = 'RoBERTA_mining.graphml'\n",
    "nx.write_graphml(largest_weak_subgraph, graphml_file_path)\n",
    "selected_node = \"Tesla\"\n",
    "ancestor_nodes = list(ancestors(G, selected_node))\n",
    "descendant_nodes = list(descendants(G, selected_node))\n",
    "mini_graph = G.subgraph(ancestor_nodes + descendant_nodes + [selected_node])\n",
    "nx.write_graphml(mini_graph, \"RoBERTA_tesla_mining.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
