{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, disable_caching\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import set_seed\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../modules'))\n",
    "from llm.model import LLMModel\n",
    "from llm.context import LLMChatContext\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# seeds = [random.randint(0, 1e9) for _ in range(5)]\n",
    "# seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"../../datasets/ManualDataset\")\n",
    "df_train = ds[\"train\"].to_pandas()\n",
    "df_valid = ds[\"valid\"].to_pandas()\n",
    "df_test = ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = os.environ.get(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If access token is not set, will raise an error. Look at the readme to obtain the access token.\n",
    "assert ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f653074d345baa8db38758e243cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "gen = LLMModel.from_transformers(\n",
    "    model_name,\n",
    "    model_kwargs={\"token\": ACCESS_TOKEN},\n",
    "    tokenizer_kwargs={\"token\": ACCESS_TOKEN},\n",
    ")\n",
    "ctx = LLMChatContext(tokenizer=gen.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_number(letter):\n",
    "    return ord(letter.upper()) - ord('A')\n",
    "def number_to_letter(number):\n",
    "    return chr(number + ord('A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"You are an AI language model trained to infer relationships between entities in a sentence. You will be provided with a masked sentence containing placeholders __NE_FROM__, __NE_TO__, and __NE_OTHER__, which represent the names of companies. Your task is to determine the type of relationship between the organizational entities __NE_FROM__ and __NE_TO__ from the following choices:\n",
    "\n",
    "A) No relationship: Indicates that there is no significant business or operational relationship between __NE_FROM__ and __NE_TO__.\n",
    "B) __NE_TO__ supplies __NE_FROM__: Indicates that __NE_TO__ supplies goods or services to __NE_FROM__.\n",
    "C) __NE_FROM__ supplies __NE_TO__: Indicates that __NE_FROM__ supplies goods or services to __NE_TO__.\n",
    "D) Ambiguous / undirected: Indicates that __NE_FROM__ and __NE_TO__ are in some form of relationship but not implying a supplier-consumer relationship, or the relationship is not clearly defined in the given context.\n",
    "E) Ownership / part-of: Indicates that __NE_FROM__ owns __NE_TO__ or is owned by __NE_TO__.\n",
    "\n",
    "Your answer should be formatted such as:\n",
    "```answer\n",
    "Answer: A\n",
    "Answer: B\n",
    "Answer: C\n",
    "Answer: D\n",
    "Answer: E\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_generator(sentence):\n",
    "    return f\"\"\"Sentence: {sentence}\n",
    "Question: What can be inferred about the relationship between the entities __NE_FROM__ and __NE_TO__ from the sentence above?\n",
    "\n",
    "A) No relationship\n",
    "B) __NE_TO__ supplies __NE_FROM__\n",
    "C) __NE_FROM__ supplies __NE_TO__\n",
    "D) Ambiguous / undirected\n",
    "E) Ownership / part-of\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, fewshot=False):\n",
    "    set_seed(seed)\n",
    "    target_box = df_test[\"label\"].values\n",
    "    result_box = []\n",
    "    for i, row in tqdm(df_test.iterrows(), total=len(df_test), desc=f\"Seed {seed}\"):\n",
    "        ctx.clear()\n",
    "        ctx.add_chat(\"system\", SYSTEM_MESSAGE)\n",
    "        if fewshot:\n",
    "            row_0 = df_train[df_train[\"label\"] == 0].sample(2)\n",
    "            row_1 = df_train[df_train[\"label\"] == 1].sample(2)\n",
    "            row_2 = df_train[df_train[\"label\"] == 2].sample(2)\n",
    "            row_3 = df_train[df_train[\"label\"] == 3].sample(2)\n",
    "            row_4 = df_train[df_train[\"label\"] == 4].sample(2)\n",
    "            samples = pd.concat([row_0, row_1, row_2, row_3, row_4])\n",
    "            samples = samples.sample(frac=1).reset_index(drop=True)\n",
    "            for _, sample in samples.iterrows():\n",
    "                question = question_generator(sample[\"masked_text\"])\n",
    "                ctx.add_chat(\"user\", question)\n",
    "                ctx.add_chat(\"assistant\", f\"Answer: {number_to_letter(sample['label'])}\")\n",
    "            if i % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        ctx.add_chat(\"user\", question_generator(row[\"masked_text\"]))\n",
    "        result = ctx.choice(prefill=\"Answer:\", choice=[\" A\", \" B\", \" C\", \" D\", \" E\"], gen=gen)\n",
    "        result_box.append(letter_to_number(result.strip()))\n",
    "    result_box_np = np.array(result_box)\n",
    "    # Calculate F1 scores\n",
    "    f1_micro = f1_score(target_box, result_box_np, average='micro')\n",
    "    f1_macro = f1_score(target_box, result_box_np, average='macro')\n",
    "    f1_classwise = f1_score(target_box, result_box_np, average=None)\n",
    "    return {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        **{f\"f1_class_{i}\": score for i, score in enumerate(f1_classwise)}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316afd8bb9914eb1aacff84ffb3792ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 992337557:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41bef1b0fa74f99b8568dfb3ca45119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 534658016:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe355ed9ca640fda4a82de5e2f04f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 24606665:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532cb1e8ab774e398c44d1da65001771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 558372984:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c83f2ef48e48898e62643c1d484739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 588628665:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Results across all runs:\n",
      "Average f1_micro: 0.4523 ± 0.0000\n",
      "Average f1_macro: 0.4666 ± 0.0000\n",
      "Average f1_class_0: 0.2840 ± 0.0000\n",
      "Average f1_class_1: 0.3923 ± 0.0000\n",
      "Average f1_class_2: 0.6378 ± 0.0000\n",
      "Average f1_class_3: 0.3277 ± 0.0000\n",
      "Average f1_class_4: 0.6914 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment with 5 different seeds\n",
    "seeds = [992337557, 534658016, 24606665, 558372984, 588628665]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, False)\n",
    "    all_results.append(results)\n",
    "\n",
    "# Calculate mean and std of F1 scores\n",
    "metrics = [\"f1_micro\", \"f1_macro\"] + [f\"f1_class_{i}\" for i in range(5)]\n",
    "\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in all_results]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"\\nAverage Results across all runs:\")\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ce5c0438de422eb0966b0332d060c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 435438698:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a780946f685c465d9e8be616620f1ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 288373858:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4564d4d4eefe491aa526432bc98a89ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 454024408:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae594fa563143479aafa91ba77955fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 406374547:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de24f707d786461eb4b7497787518c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Seed 923762016:   0%|          | 0/745 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Results across all runs:\n",
      "Average f1_micro: 0.5270 ± 0.0092\n",
      "Average f1_macro: 0.5409 ± 0.0094\n",
      "Average f1_class_0: 0.4413 ± 0.0198\n",
      "Average f1_class_1: 0.4859 ± 0.0115\n",
      "Average f1_class_2: 0.6190 ± 0.0099\n",
      "Average f1_class_3: 0.4701 ± 0.0265\n",
      "Average f1_class_4: 0.6886 ± 0.0223\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment with 5 different seeds\n",
    "seeds = [435438698, 288373858, 454024408, 406374547, 923762016]\n",
    "all_results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, True)\n",
    "    all_results.append(results)\n",
    "\n",
    "# Calculate mean and std of F1 scores\n",
    "metrics = [\"f1_micro\", \"f1_macro\"] + [f\"f1_class_{i}\" for i in range(5)]\n",
    "\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in all_results]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"\\nAverage Results across all runs:\")\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
