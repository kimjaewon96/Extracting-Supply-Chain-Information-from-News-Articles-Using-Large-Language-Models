{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "ManualDataset requires the original dataset from [Wichmann et al.](https://github.com/pwichmann/supply_chain_mining). To obtain the dataset, please go to the following link (https://github.com/pwichmann/supply_chain_mining) and contact the author. When you have successfully obtained the dataset, move the 'training_data' folder of the dataset to the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ftfy\n",
    "import os\n",
    "import re\n",
    "from datasets import DatasetDict, Dataset\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 3.9/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ------- 10.5/12.8 MB 14.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 15.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.0)\n",
      "Requirement already satisfied: jinja2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (74.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: tzdata in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\iitp_paper_code\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def split_sentences(v):\n",
    "    return [x.text for x in nlp(v).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "original_dataset_path = Path(cwd, \"../../training_data\").resolve()\n",
    "assert original_dataset_path.is_dir()\n",
    "original_dataset_train_path = original_dataset_path / \"01_initial_corpus\" / \"train.json\"\n",
    "assert original_dataset_train_path.is_file()\n",
    "original_dataset_valid_path = original_dataset_path / \"01_initial_corpus\" / \"dev.json\"\n",
    "assert original_dataset_valid_path.is_file()\n",
    "original_dataset_test_path = original_dataset_path / \"01_initial_corpus\" / \"test.json\"\n",
    "assert original_dataset_test_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(v):\n",
    "    v = ftfy.fix_text(v)\n",
    "    v = v.replace(\"\\xa0\", \"\").replace(\"\\n\", \" \").replace(\"â€”\", \"-\")\n",
    "    v = re.sub(\n",
    "        r\"\\(.*?\\)|\\[.*?\\]|<.*?>|\\{.*?\\}\", \"\", v\n",
    "    )\n",
    "    v = re.sub(r\"\\s{2,}\", \" \", v).strip()\n",
    "    return v\n",
    "\n",
    "def whitespace_tokenizer(sentence):\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence, re.UNICODE)\n",
    "    spaces = []\n",
    "    s = \"\"\n",
    "    for token in tokens:\n",
    "        if s == \"\":\n",
    "            s = token\n",
    "            continue\n",
    "        c = sentence[len(s)]\n",
    "        if c == \" \":\n",
    "            spaces.append(1)\n",
    "            s += \" \" + token\n",
    "        else:\n",
    "            spaces.append(0)\n",
    "            s += token\n",
    "    return tokens, spaces\n",
    "\n",
    "\n",
    "def reconstruct_sentence(tokens, spaces):\n",
    "    sentence = \"\"\n",
    "    for token, space in zip(tokens, spaces + [0]):\n",
    "        sentence += token + \" \" * space\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def find_ne_parts(list1, list2):\n",
    "    ne_from_indices = []\n",
    "    ne_to_indices = []\n",
    "    ne_other_indices = []\n",
    "\n",
    "    i1, i2 = 0, 0\n",
    "    while i2 < len(list2):\n",
    "        if list2[i2] == \"__NE_FROM__\":\n",
    "            i2 += 1\n",
    "            start_idx = i1\n",
    "            while i1 < len(list1) and (i2 >= len(list2) or list1[i1] != list2[i2]):\n",
    "                i1 += 1\n",
    "            ne_from_indices.append((start_idx, i1))\n",
    "        elif list2[i2] == \"__NE_TO__\":\n",
    "            i2 += 1\n",
    "            start_idx = i1\n",
    "            while i1 < len(list1) and (i2 >= len(list2) or list1[i1] != list2[i2]):\n",
    "                i1 += 1\n",
    "            ne_to_indices.append((start_idx, i1))\n",
    "        elif list2[i2] == \"__NE_OTHER__\":\n",
    "            i2 += 1\n",
    "            start_idx = i1\n",
    "            while i1 < len(list1) and (i2 >= len(list2) or list1[i1] != list2[i2]):\n",
    "                i1 += 1\n",
    "            ne_other_indices.append((start_idx, i1))\n",
    "        else:\n",
    "            if i1 < len(list1) and list1[i1] == list2[i2]:\n",
    "                i1 += 1\n",
    "            i2 += 1\n",
    "\n",
    "    return ne_from_indices, ne_to_indices, ne_other_indices\n",
    "\n",
    "\n",
    "def reconstruct_entities(list1, spaces, indices):\n",
    "    entities = []\n",
    "    for start, end in indices:\n",
    "        entity_tokens = list1[start:end]\n",
    "        entity_spaces = spaces[start : end - 1] if end > start else []\n",
    "        entity = reconstruct_sentence(entity_tokens, entity_spaces)\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def get_mask_order(sentence: str):\n",
    "    words = sentence.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if '__NE_FROM__' in word or '__NE_TO__' in word or '__NE_OTHER__' in word:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "\n",
    "def reconstruct_masked_sentence(\n",
    "    original_sentence: str, ne_from: list[str], ne_to: list[str], ne_other: list[str], mask_order: list[str]\n",
    "):\n",
    "    masked_sentence = original_sentence\n",
    "    q_ne_from = ne_from.copy()\n",
    "    q_ne_to = ne_to.copy()\n",
    "    q_ne_other = ne_other.copy()\n",
    "    q_mask_order = mask_order.copy()\n",
    "    while len(q_mask_order) > 0:\n",
    "        #print(masked_sentence, q_ne_from, q_ne_to, q_ne_other, q_mask_order)\n",
    "        if q_mask_order[0] == \"__NE_FROM__\":\n",
    "            assert masked_sentence.replace(q_ne_from[0], '__NE_FROM__', 1) != masked_sentence\n",
    "            masked_sentence = masked_sentence.replace(q_ne_from[0], '__NE_FROM__', 1)\n",
    "            q_ne_from.pop(0)\n",
    "        elif q_mask_order[0] == \"__NE_TO__\":\n",
    "            assert masked_sentence.replace(q_ne_to[0], '__NE_TO__', 1) != masked_sentence\n",
    "            masked_sentence = masked_sentence.replace(q_ne_to[0], '__NE_TO__', 1)\n",
    "            q_ne_to.pop(0)\n",
    "        elif q_mask_order[0] == \"__NE_OTHER__\":\n",
    "            assert masked_sentence.replace(q_ne_other[0], '__NE_OTHER__', 1) != masked_sentence\n",
    "            masked_sentence = masked_sentence.replace(q_ne_other[0], '__NE_OTHER__', 1)\n",
    "            q_ne_other.pop(0)\n",
    "        q_mask_order.pop(0)\n",
    "    return masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    processed_data = []\n",
    "    for _, row in pd.read_json(file_path).iterrows():\n",
    "        original_text = fix_text(row[\"originalText\"])\n",
    "        for relation in row[\"relations\"].values():\n",
    "            masked_text = fix_text(relation[\"x\"])\n",
    "            if \"__NE_FROM__\" in masked_text and \"__NE_TO__\" in masked_text:\n",
    "                processed_data.append(\n",
    "                    {\n",
    "                        \"original_text\": original_text,\n",
    "                        \"masked_text\": masked_text,\n",
    "                        \"label\": relation[\"y\"],\n",
    "                    }\n",
    "                )\n",
    "    df_processed = pd.DataFrame(processed_data)\n",
    "    final_data = []\n",
    "    for _, row in tqdm(df_processed.iterrows(), total=len(df_processed)):\n",
    "        o_text_list, o_text_spaces = whitespace_tokenizer(row[\"original_text\"])\n",
    "        m_text_list, _ = whitespace_tokenizer(row[\"masked_text\"])\n",
    "        from_indices, to_indices, other_indices = find_ne_parts(\n",
    "            o_text_list, m_text_list\n",
    "        )\n",
    "        NE_FROM = reconstruct_entities(o_text_list, o_text_spaces, from_indices)\n",
    "        NE_TO = reconstruct_entities(o_text_list, o_text_spaces, to_indices)\n",
    "        NE_OTHER = reconstruct_entities(o_text_list, o_text_spaces, other_indices)\n",
    "        if len(NE_FROM) != 1 or len(NE_TO) != 1:\n",
    "            continue\n",
    "        if NE_FROM[0] == \"\" or NE_TO[0] == \"\" or NE_FROM[0] == NE_TO[0]:\n",
    "            continue\n",
    "        if len(split_sentences(row[\"original_text\"])) > 1:\n",
    "            continue\n",
    "        mask_order = get_mask_order(row['masked_text'])\n",
    "        try:\n",
    "            row_masked_text = reconstruct_masked_sentence(\n",
    "                row[\"original_text\"], NE_FROM, NE_TO, NE_OTHER, mask_order\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "        final_data.append(\n",
    "            {\n",
    "                \"original_text\": row[\"original_text\"],\n",
    "                \"masked_text\": row_masked_text,\n",
    "                \"label\": row[\"label\"],\n",
    "                \"NE_FROM\": NE_FROM[0],\n",
    "                \"NE_TO\": NE_TO[0],\n",
    "                \"NE_OTHER\": NE_OTHER,\n",
    "            }\n",
    "        )\n",
    "        df_final = pd.DataFrame(final_data)\n",
    "        df_final = df_final.groupby('original_text').filter(\n",
    "            lambda x: (x['label'] != 0).any()\n",
    "        )\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e972515528194981bb6a7e57b7eba503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b5921d209e4ac0b6ad6b7e0637e35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/758 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a304382831471fbe1cb3e66e737fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = preprocess(original_dataset_train_path)\n",
    "df_train[\"source\"] = \"train\"\n",
    "df_valid = preprocess(original_dataset_valid_path)\n",
    "df_valid[\"source\"] = \"valid\"\n",
    "df_test = preprocess(original_dataset_test_path)\n",
    "df_test[\"source\"] = \"test\"\n",
    "concatenated_df = pd.concat([df_train, df_valid, df_test])\n",
    "duplicates = concatenated_df[concatenated_df.duplicated(\"masked_text\", keep=False)]\n",
    "conflicting_duplicates = duplicates.groupby(\"masked_text\").filter(\n",
    "    lambda x: x[\"label\"].nunique() > 1\n",
    ")\n",
    "df_ds = concatenated_df[\n",
    "    ~concatenated_df.index.isin(conflicting_duplicates.index)\n",
    "]\n",
    "df_ds = df_ds.drop_duplicates(subset=[\"masked_text\"], keep=\"first\")\n",
    "ds = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(\n",
    "            df_ds[df_ds[\"source\"] == \"train\"]\n",
    "        ),\n",
    "        \"valid\": Dataset.from_pandas(\n",
    "            df_ds[df_ds[\"source\"] == \"valid\"]\n",
    "        ),\n",
    "        \"test\": Dataset.from_pandas(\n",
    "            df_ds[df_ds[\"source\"] == \"test\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>masked_text</th>\n",
       "      <th>label</th>\n",
       "      <th>NE_FROM</th>\n",
       "      <th>NE_TO</th>\n",
       "      <th>NE_OTHER</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Officials of the Naval Air Systems Command at ...</td>\n",
       "      <td>Officials of the __NE_FROM__ at Patuxent River...</td>\n",
       "      <td>3</td>\n",
       "      <td>Naval Air Systems Command</td>\n",
       "      <td>Cobham plc Advanced Electronic Solutions</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UTC Aerospace Systems has been selected by The...</td>\n",
       "      <td>__NE_FROM__ has been selected by The __NE_OTHE...</td>\n",
       "      <td>2</td>\n",
       "      <td>UTC Aerospace Systems</td>\n",
       "      <td>Boeing</td>\n",
       "      <td>[Boeing Co.]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UTC Aerospace Systems has been selected by The...</td>\n",
       "      <td>__NE_OTHER__ has been selected by The __NE_FRO...</td>\n",
       "      <td>0</td>\n",
       "      <td>Boeing Co.</td>\n",
       "      <td>Boeing</td>\n",
       "      <td>[UTC Aerospace Systems]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UTC Aerospace Systems has been selected by The...</td>\n",
       "      <td>__NE_FROM__ has been selected by The __NE_TO__...</td>\n",
       "      <td>2</td>\n",
       "      <td>UTC Aerospace Systems</td>\n",
       "      <td>Boeing Co.</td>\n",
       "      <td>[Boeing]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tenneco Automotive said Tuesday one of its uni...</td>\n",
       "      <td>__NE_FROM__ said Tuesday one of its units had ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Tenneco Automotive</td>\n",
       "      <td>Minuzzi</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Lockheed Martin received a $769.5 million modi...</td>\n",
       "      <td>__NE_OTHER__ received a $769.5 million modific...</td>\n",
       "      <td>0</td>\n",
       "      <td>Navy</td>\n",
       "      <td>Defense</td>\n",
       "      <td>[Lockheed Martin, USAF, USMC]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>Lockheed Martin received a $769.5 million modi...</td>\n",
       "      <td>__NE_OTHER__ received a $769.5 million modific...</td>\n",
       "      <td>0</td>\n",
       "      <td>USAF</td>\n",
       "      <td>Defense</td>\n",
       "      <td>[Lockheed Martin, USMC, Navy]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>In April, HAECO Cabin Solutions signed a contr...</td>\n",
       "      <td>In April, __NE_FROM__ signed a contract with _...</td>\n",
       "      <td>3</td>\n",
       "      <td>HAECO Cabin Solutions</td>\n",
       "      <td>Airbus</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>HITCO to Supply Composite Components for Boeing</td>\n",
       "      <td>__NE_FROM__ to Supply Composite Components for...</td>\n",
       "      <td>2</td>\n",
       "      <td>HITCO</td>\n",
       "      <td>Boeing</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>Aerolineas Argentinas received on 26 January t...</td>\n",
       "      <td>__NE_FROM__ received on 26 January the third o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Aerolineas Argentinas</td>\n",
       "      <td>Boeing</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3722 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_text  \\\n",
       "2     Officials of the Naval Air Systems Command at ...   \n",
       "3     UTC Aerospace Systems has been selected by The...   \n",
       "4     UTC Aerospace Systems has been selected by The...   \n",
       "5     UTC Aerospace Systems has been selected by The...   \n",
       "6     Tenneco Automotive said Tuesday one of its uni...   \n",
       "...                                                 ...   \n",
       "1257  Lockheed Martin received a $769.5 million modi...   \n",
       "1258  Lockheed Martin received a $769.5 million modi...   \n",
       "1261  In April, HAECO Cabin Solutions signed a contr...   \n",
       "1262    HITCO to Supply Composite Components for Boeing   \n",
       "1263  Aerolineas Argentinas received on 26 January t...   \n",
       "\n",
       "                                            masked_text  label  \\\n",
       "2     Officials of the __NE_FROM__ at Patuxent River...      3   \n",
       "3     __NE_FROM__ has been selected by The __NE_OTHE...      2   \n",
       "4     __NE_OTHER__ has been selected by The __NE_FRO...      0   \n",
       "5     __NE_FROM__ has been selected by The __NE_TO__...      2   \n",
       "6     __NE_FROM__ said Tuesday one of its units had ...      4   \n",
       "...                                                 ...    ...   \n",
       "1257  __NE_OTHER__ received a $769.5 million modific...      0   \n",
       "1258  __NE_OTHER__ received a $769.5 million modific...      0   \n",
       "1261  In April, __NE_FROM__ signed a contract with _...      3   \n",
       "1262  __NE_FROM__ to Supply Composite Components for...      2   \n",
       "1263  __NE_FROM__ received on 26 January the third o...      1   \n",
       "\n",
       "                        NE_FROM                                     NE_TO  \\\n",
       "2     Naval Air Systems Command  Cobham plc Advanced Electronic Solutions   \n",
       "3         UTC Aerospace Systems                                    Boeing   \n",
       "4                    Boeing Co.                                    Boeing   \n",
       "5         UTC Aerospace Systems                                Boeing Co.   \n",
       "6            Tenneco Automotive                                   Minuzzi   \n",
       "...                         ...                                       ...   \n",
       "1257                       Navy                                   Defense   \n",
       "1258                       USAF                                   Defense   \n",
       "1261      HAECO Cabin Solutions                                    Airbus   \n",
       "1262                      HITCO                                    Boeing   \n",
       "1263      Aerolineas Argentinas                                    Boeing   \n",
       "\n",
       "                           NE_OTHER source  \n",
       "2                                []  train  \n",
       "3                      [Boeing Co.]  train  \n",
       "4           [UTC Aerospace Systems]  train  \n",
       "5                          [Boeing]  train  \n",
       "6                                []  train  \n",
       "...                             ...    ...  \n",
       "1257  [Lockheed Martin, USAF, USMC]   test  \n",
       "1258  [Lockheed Martin, USMC, Navy]   test  \n",
       "1261                             []   test  \n",
       "1262                             []   test  \n",
       "1263                             []   test  \n",
       "\n",
       "[3722 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3722"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ds[\"original_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1940"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ds[\"original_text\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7bcf66759c420bb04711f09d2907d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f7ca0462ef4d188a022370eceeead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd15d45c7225467db18cbf80fbfbab32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk(str(Path(cwd, \"../../datasets/ManualDataset\").resolve()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IITP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
