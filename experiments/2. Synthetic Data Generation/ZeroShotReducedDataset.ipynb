{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "from itertools import combinations\n",
    "from time import sleep\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import set_seed, pipeline\n",
    "from datasets import disable_caching, load_from_disk, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../modules'))\n",
    "from llm.context import LLMChatContext\n",
    "from llm.model import LLMModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "disable_caching()\n",
    "df_train = load_from_disk(\"../../datasets/ManualReducedDataset\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = os.environ.get(\"ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If access token is not set, will raise an error. Look at the readme to obtain the access token.\n",
    "assert ACCESS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c36dc9df3d4025baf487c939e9a3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "gen = LLMModel.from_transformers(\n",
    "    model_name,\n",
    "    model_kwargs={\"token\": ACCESS_TOKEN, \"max_length\": 4096},\n",
    "    tokenizer_kwargs={\"token\": ACCESS_TOKEN},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(A:str, B:str, label: int):\n",
    "    match label:\n",
    "        case 1:\n",
    "            user_prompt = f\"Generate a single sentence that imply a buyer-supplier relationship between {A} and {B}, where {A} is the buyer and {B} is the supplier.\" # Note that the sentences should not portray an ambiguous partnership relationship between {A} and {B}. The sentence should portray a clear indication of a directed supply chain relationship.\n",
    "        case 2:\n",
    "            user_prompt = f\"Generate a single sentence that imply a supplier-buyer relationship between {A} and {B}, where {A} is the supplier and {B} is the buyer.\" #,  Note that the sentences should not portray an ambiguous partnership relationship between {A} and {B}. The sentence should portray a clear indication of a directed supply chain relationship. such as expressing the items that are shipped by {A} to {B}.\n",
    "        case 3:\n",
    "            user_prompt = f\"Generate a single sentence that imply a relationship between {A} and {B} that is portrayed as arbitrary or undirected.\" # This relationship can take various forms, such as collaborations, joint ventures, strategic alliances, or any other type of ambiguous business relationship. Note that the sentence should not convey a supply chain relationship between the two companies.\n",
    "        case 4:\n",
    "            user_prompt = f\"Generate a single sentence that imply an ownership relationship between {A} and {B}, where {A} owns or is owned by {B}.\"\n",
    "    #user_prompt += \"\"\"\\nThe sentence should meet the following criteria:\n",
    "#- Crafted in a style that could be reminiscent of a newspaper article, press release, industry report, or any other relevant source.\n",
    "#- Should be unique, diverse and creative.\"\"\"\n",
    "#- Contain the exact entity words '{A}' and '{B}'.\n",
    "    return user_prompt\n",
    "\n",
    "def generate_sentences(label: int, shots: int = 10):\n",
    "    totals = df_train[df_train['label'] == label].sample(shots+1)\n",
    "    samples = totals.iloc[:shots]\n",
    "    target = totals.iloc[shots]\n",
    "    A = target['NE_FROM']\n",
    "    B = target['NE_TO']\n",
    "    if random.random() > 0.5:\n",
    "        C = A\n",
    "        A = B\n",
    "        B = C\n",
    "        if label == 2:\n",
    "            label = 1\n",
    "        elif label == 1:\n",
    "            label = 2\n",
    "    save_dict = {\n",
    "        \"A\": A,\n",
    "        \"B\": B,\n",
    "        \"label\": label,\n",
    "        \"sentences\": []\n",
    "    }\n",
    "    ctx = LLMChatContext(tokenizer=gen.tokenizer)\n",
    "    ctx.add_chat(role=\"system\", content=\"\"\"You are a helpful AI assistant with extensive knowledge of the global supply chain. Your task is to generate a realistic yet unique sentence that matches the given context. The sentence should be crafted in a style that could be reminiscent of a newspaper article, press release, industry report, or any other relevant source. Please provide your response in the JSON format below:\n",
    "```json\n",
    "{{\n",
    "    \"sentence\": \\\"(example sentence)\\\"\n",
    "}}\n",
    "```\"\"\")\n",
    "#     for _, sample in samples.iterrows():\n",
    "#         ctx.add_chat(role=\"user\", content=generate_question(A=sample['NE_FROM'], B=sample['NE_TO'], label=sample['label']))\n",
    "#         ctx.add_chat(role=\"assistant\", content=f\"\"\"```json\n",
    "# {{\n",
    "#     \"sentence\": \\\"{sample[\"original_text\"]}\\\"\n",
    "# }}\n",
    "# ```\"\"\")\n",
    "    ctx.add_chat(role=\"user\", content=generate_question(A=target['NE_FROM'], B=target['NE_TO'], label=target['label']))\n",
    "    #print(ctx.render())\n",
    "    response = ctx.generate(prefill=\"```json\", gen=gen, generation_config={\"do_sample\": True, \"top_p\": 0.95, \"stop_strings\": \"}\"})\n",
    "    try:\n",
    "        obj = ast.literal_eval(response + \"}\")\n",
    "        save_dict[\"sentences\"].append(obj['sentence'])\n",
    "    except Exception:\n",
    "        pass\n",
    "    del ctx\n",
    "    return save_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc84ce602b14d04adff35cc57a38e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "results = []\n",
    "for idx in tqdm(range(700)):\n",
    "    results.append(generate_sentences(label=1))\n",
    "    results.append(generate_sentences(label=2))\n",
    "    results.append(generate_sentences(label=3))\n",
    "    results.append(generate_sentences(label=4))\n",
    "    pd.DataFrame(results).to_json(\"2.Zero_shot_Synthetic_Data.json\", orient=\"records\", force_ascii=False)\n",
    "    if idx % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.unload()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_parentheses(row):\n",
    "    row[\"original_text\"] = re.sub(r'\\(.*?\\)|\\[.*?\\]|<.*?>|\\{.*?\\}', '', row[\"original_text\"])\n",
    "    row[\"original_text\"] = re.sub(r'\\s{2,}', ' ', row[\"original_text\"]).strip()\n",
    "    return row\n",
    "\n",
    "def text_generator(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield row[\"original_text\"]\n",
    "\n",
    "def process_named_entities(df, ner_pipeline):\n",
    "    results = []\n",
    "    for output in tqdm(ner_pipeline(text_generator(df), aggregation_strategy=\"first\", batch_size=256)):\n",
    "        org_entities = [x for x in output if x['entity_group'] == 'ORG']\n",
    "        results.append(org_entities if len(org_entities) >= 2 else [])\n",
    "    return results\n",
    "\n",
    "def create_entity_pairs(df, ner_results):\n",
    "    entity_pairs = []\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "        output = ner_results[idx]\n",
    "        for token_idx_from, token_idx_to in combinations(range(len(output)), r=2):\n",
    "            pair = create_entity_pair(row, output, token_idx_from, token_idx_to)\n",
    "            entity_pairs.append(pair)\n",
    "    return entity_pairs\n",
    "\n",
    "def create_entity_pair(row, entities, idx_from, idx_to):\n",
    "    pair = {\n",
    "        \"original_text\": row['original_text'],\n",
    "        \"label\": None,\n",
    "        \"NE_FROM\": None,\n",
    "        \"NE_TO\": None,\n",
    "        \"NE_OTHER\": [],\n",
    "        \"masked_text\": row[\"original_text\"]\n",
    "    }\n",
    "\n",
    "    for token_idx, token in reversed(list(enumerate(entities))):\n",
    "        if token_idx == idx_to:\n",
    "            pair[\"NE_TO\"] = token['word']\n",
    "            pair[\"masked_text\"] = pair[\"masked_text\"][:token['start']] + \"__NE_TO__\" + pair[\"masked_text\"][token['end']:]\n",
    "        elif token_idx == idx_from:\n",
    "            pair[\"NE_FROM\"] = token['word']\n",
    "            pair[\"masked_text\"] = pair[\"masked_text\"][:token['start']] + \"__NE_FROM__\" + pair[\"masked_text\"][token['end']:]\n",
    "        else:\n",
    "            pair[\"NE_OTHER\"].append(token['word'])\n",
    "            pair[\"masked_text\"] = pair[\"masked_text\"][:token['start']] + \"__NE_OTHER__\" + pair[\"masked_text\"][token['end']:]\n",
    "\n",
    "    pair['label'] = determine_label(row, pair[\"NE_FROM\"], pair[\"NE_TO\"])\n",
    "    return pair\n",
    "\n",
    "def determine_label(row, ne_from, ne_to):\n",
    "    if ne_from == row['A'] and ne_to == row['B']:\n",
    "        return row['label']\n",
    "    elif ne_from == row['B'] and ne_to == row['A']:\n",
    "        if row['label'] == 1:\n",
    "            return 2\n",
    "        elif row['label'] == 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return row['label']\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead49f5f144e4f8faf48d2a113ef7c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81061557487943ecbf37212dbc3f86e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd84cc7c57e489b84665b93f71283af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df.explode(\"sentences\").dropna().reset_index(drop=True)\n",
    "df = df.rename(columns={\"sentences\": \"original_text\"})\n",
    "df = df.apply(strip_parentheses, axis=1)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-large-NER\", device=\"cuda\")\n",
    "ner_results = process_named_entities(df, ner_pipeline)\n",
    "entity_pairs = create_entity_pairs(df, ner_results)\n",
    "final_df = pd.DataFrame(entity_pairs)\n",
    "final_df = final_df.groupby('original_text').filter(\n",
    "    lambda x: (x['NE_FROM'] != '').all() and\n",
    "              (x['NE_TO'] != '').all() and\n",
    "              (x['label'] != 0).any()\n",
    ").reset_index(drop=True)\n",
    "Dataset.from_pandas(final_df).save_to_disk(\"../../datasets/ZeroShotReducedDataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
