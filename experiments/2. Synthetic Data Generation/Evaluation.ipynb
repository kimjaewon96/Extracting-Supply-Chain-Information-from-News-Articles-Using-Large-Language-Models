{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, disable_caching, concatenate_datasets\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../modules'))\n",
    "from experiment_1.RoBERTaEntity import RoBERTaEntity\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[900263936, 237090229, 900569911, 553032617, 115865595]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "seeds = [random.randint(0, 1e9) for _ in range(5)]\n",
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "id2label = {\n",
    "    0: \"reject\",\n",
    "    1: \"B_supplies_A\",\n",
    "    2: \"A_supplies_B\",\n",
    "    3: \"ambiguous\",\n",
    "    4: \"ownership\",\n",
    "}\n",
    "label2id = {\n",
    "    \"reject\": 0,\n",
    "    \"B_supplies_A\": 1,\n",
    "    \"A_supplies_B\": 2,\n",
    "    \"ambiguous\": 3,\n",
    "    \"ownership\": 4,\n",
    "}\n",
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"__NE_FROM__\", \"__NE_TO__\", \"__NE_OTHER__\"]}\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_micro = f1_score(labels, predictions, average='micro')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_classwise = f1_score(labels, predictions, average=None)\n",
    "\n",
    "    return {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        **{f\"f1_class_{i}\": score for i, score in enumerate(f1_classwise)}\n",
    "    }\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    model = RoBERTaEntity.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=5,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ff26e6a05f4f68920d620585d774ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810ea649eeb24feeb92524aee4bf52a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2954 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba7fbcf41834419865f86eec7f28638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc125abab3d54152ab5a4a3d10fbd093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880612d6034e4e43bbeb6ad330b76022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a893f2c15147ffbdfe78492625a90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_zero = load_from_disk(\"../../datasets/ZeroShotDataset\")\n",
    "ds_zero = ds_zero.select_columns([\"masked_text\", \"label\"])\n",
    "ds_zero = ds_zero.rename_column(\"masked_text\", \"text\")\n",
    "ds_zero = ds_zero.map(preprocess_function, batched=True)\n",
    "\n",
    "ds_few = load_from_disk(\"../../datasets/TenShotDataset\")\n",
    "ds_few = ds_few.select_columns([\"masked_text\", \"label\"])\n",
    "ds_few = ds_few.rename_column(\"masked_text\", \"text\")\n",
    "ds_few = ds_few.map(preprocess_function, batched=True)\n",
    "\n",
    "ds_two = load_from_disk(\"../../datasets/TwoStageDataset\")\n",
    "ds_two = ds_two.select_columns([\"masked_text\", \"label\"])\n",
    "ds_two = ds_two.rename_column(\"masked_text\", \"text\")\n",
    "ds_two = ds_two.map(preprocess_function, batched=True)\n",
    "\n",
    "ds = load_from_disk(\"../../datasets/ManualDataset\")\n",
    "ds = ds.select_columns([\"masked_text\", \"label\"])\n",
    "ds = ds.rename_column(\"masked_text\", \"text\")\n",
    "ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed, train_dataset, output_dir=\"logs/experiment_2\"):\n",
    "    set_seed(seed)\n",
    "    training_args = TrainingArguments(\n",
    "        seed=seed,\n",
    "        data_seed=seed,\n",
    "        tf32=True,\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        warmup_ratio=0.1,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,\n",
    "        report_to=[],\n",
    "        save_only_model=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=ds[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    test_results = trainer.predict(ds[\"test\"])\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroShotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.5431 ± 0.0295\n",
      "Average test_f1_macro: 0.4733 ± 0.0318\n",
      "Average test_f1_class_0: 0.6474 ± 0.0221\n",
      "Average test_f1_class_1: 0.4486 ± 0.0641\n",
      "Average test_f1_class_2: 0.5203 ± 0.0772\n",
      "Average test_f1_class_3: 0.2480 ± 0.0415\n",
      "Average test_f1_class_4: 0.5021 ± 0.0387\n"
     ]
    }
   ],
   "source": [
    "seeds = [115624676, 313063585, 725591958, 966378087, 644779121]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_zero)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TenShotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.6239 ± 0.0277\n",
      "Average test_f1_macro: 0.5753 ± 0.0340\n",
      "Average test_f1_class_0: 0.6927 ± 0.0143\n",
      "Average test_f1_class_1: 0.5756 ± 0.0189\n",
      "Average test_f1_class_2: 0.6093 ± 0.0729\n",
      "Average test_f1_class_3: 0.4202 ± 0.0778\n",
      "Average test_f1_class_4: 0.5787 ± 0.0398\n"
     ]
    }
   ],
   "source": [
    "seeds = [764077453, 710632782, 865707432, 482052655, 452353057]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_few)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TwoStageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.5611 ± 0.0314\n",
      "Average test_f1_macro: 0.5249 ± 0.0249\n",
      "Average test_f1_class_0: 0.6402 ± 0.0348\n",
      "Average test_f1_class_1: 0.4791 ± 0.0915\n",
      "Average test_f1_class_2: 0.5472 ± 0.0284\n",
      "Average test_f1_class_3: 0.3940 ± 0.0399\n",
      "Average test_f1_class_4: 0.5640 ± 0.0472\n"
     ]
    }
   ],
   "source": [
    "seeds = [624420665, 719682587, 749390431, 356261854, 724384563]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_two)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualDataset + ZeroShotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.7936 ± 0.0107\n",
      "Average test_f1_macro: 0.7867 ± 0.0122\n",
      "Average test_f1_class_0: 0.7926 ± 0.0185\n",
      "Average test_f1_class_1: 0.8202 ± 0.0235\n",
      "Average test_f1_class_2: 0.8259 ± 0.0088\n",
      "Average test_f1_class_3: 0.6668 ± 0.0148\n",
      "Average test_f1_class_4: 0.8278 ± 0.0188\n"
     ]
    }
   ],
   "source": [
    "seeds = [230818434, 598147794, 339224527, 228940900, 111815430]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_zero, ds['train']]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualDataset + TenShotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.7981 ± 0.0112\n",
      "Average test_f1_macro: 0.7816 ± 0.0109\n",
      "Average test_f1_class_0: 0.8160 ± 0.0100\n",
      "Average test_f1_class_1: 0.7951 ± 0.0194\n",
      "Average test_f1_class_2: 0.8366 ± 0.0110\n",
      "Average test_f1_class_3: 0.6714 ± 0.0272\n",
      "Average test_f1_class_4: 0.7889 ± 0.0191\n"
     ]
    }
   ],
   "source": [
    "seeds = [173548468, 351960359, 597931470, 910980050, 774195904]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_few, ds['train']]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualDataset + TwoStageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.7817 ± 0.0100\n",
      "Average test_f1_macro: 0.7667 ± 0.0115\n",
      "Average test_f1_class_0: 0.7807 ± 0.0171\n",
      "Average test_f1_class_1: 0.8039 ± 0.0184\n",
      "Average test_f1_class_2: 0.8314 ± 0.0153\n",
      "Average test_f1_class_3: 0.6264 ± 0.0275\n",
      "Average test_f1_class_4: 0.7912 ± 0.0498\n"
     ]
    }
   ],
   "source": [
    "seeds = [271054931, 88464423, 222017585, 272392254, 668356033]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_two, ds['train']]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfafd6ace3c4234947652a3a6a47f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd35344b7504e09b90177ea30124d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b889c5f8947d4d9290cdb718b9903988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2699 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_reduce = load_from_disk(\"../../datasets/ManualReducedDataset\")\n",
    "ds_reduce = ds_reduce.select_columns([\"masked_text\", \"label\"])\n",
    "ds_reduce = ds_reduce.rename_column(\"masked_text\", \"text\")\n",
    "ds_reduce = ds_reduce.map(preprocess_function, batched=True)\n",
    "\n",
    "ds_zero_reduce = load_from_disk(\"../../datasets/ZeroShotReducedDataset\")\n",
    "ds_zero_reduce = ds_zero_reduce.select_columns([\"masked_text\", \"label\"])\n",
    "ds_zero_reduce = ds_zero_reduce.rename_column(\"masked_text\", \"text\")\n",
    "ds_zero_reduce = ds_zero_reduce.map(preprocess_function, batched=True)\n",
    "\n",
    "ds_few_reduce = load_from_disk(\"../../datasets/TenShotReducedDataset\")\n",
    "ds_few_reduce = ds_few_reduce.select_columns([\"masked_text\", \"label\"])\n",
    "ds_few_reduce = ds_few_reduce.rename_column(\"masked_text\", \"text\")\n",
    "ds_few_reduce = ds_few_reduce.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.5930 ± 0.0270\n",
      "Average test_f1_macro: 0.5231 ± 0.0372\n",
      "Average test_f1_class_0: 0.6801 ± 0.0140\n",
      "Average test_f1_class_1: 0.3799 ± 0.1069\n",
      "Average test_f1_class_2: 0.6292 ± 0.0365\n",
      "Average test_f1_class_3: 0.3996 ± 0.0294\n",
      "Average test_f1_class_4: 0.5266 ± 0.0298\n"
     ]
    }
   ],
   "source": [
    "seeds = [286683549, 6420921, 197067174, 959206791, 413404281]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_reduce)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZeroShotReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.5348 ± 0.0141\n",
      "Average test_f1_macro: 0.4797 ± 0.0175\n",
      "Average test_f1_class_0: 0.6535 ± 0.0132\n",
      "Average test_f1_class_1: 0.4567 ± 0.0221\n",
      "Average test_f1_class_2: 0.5069 ± 0.0524\n",
      "Average test_f1_class_3: 0.3217 ± 0.0543\n",
      "Average test_f1_class_4: 0.4598 ± 0.0351\n"
     ]
    }
   ],
   "source": [
    "seeds = [320896692, 932940650, 883472909, 968454980, 421657822]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_zero_reduce)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TenShotReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.6113 ± 0.0214\n",
      "Average test_f1_macro: 0.5394 ± 0.0244\n",
      "Average test_f1_class_0: 0.6832 ± 0.0196\n",
      "Average test_f1_class_1: 0.5462 ± 0.0421\n",
      "Average test_f1_class_2: 0.6441 ± 0.0477\n",
      "Average test_f1_class_3: 0.3175 ± 0.0485\n",
      "Average test_f1_class_4: 0.5063 ± 0.0214\n"
     ]
    }
   ],
   "source": [
    "seeds = [706375894, 14365483, 593256681, 791003381, 210000475]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, ds_few_reduce)\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualReducedDataset + ZeroShotReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.6231 ± 0.0452\n",
      "Average test_f1_macro: 0.5822 ± 0.0417\n",
      "Average test_f1_class_0: 0.6125 ± 0.1074\n",
      "Average test_f1_class_1: 0.6012 ± 0.0341\n",
      "Average test_f1_class_2: 0.7172 ± 0.0132\n",
      "Average test_f1_class_3: 0.3893 ± 0.0911\n",
      "Average test_f1_class_4: 0.5908 ± 0.0463\n"
     ]
    }
   ],
   "source": [
    "seeds = [686579303, 119540831, 26855092, 796233790, 295310485]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_zero_reduce, ds_reduce]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualReducedDataset + TenShotReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.6623 ± 0.0092\n",
      "Average test_f1_macro: 0.5905 ± 0.0083\n",
      "Average test_f1_class_0: 0.7026 ± 0.0080\n",
      "Average test_f1_class_1: 0.6466 ± 0.0115\n",
      "Average test_f1_class_2: 0.7328 ± 0.0181\n",
      "Average test_f1_class_3: 0.3661 ± 0.0413\n",
      "Average test_f1_class_4: 0.5046 ± 0.0367\n"
     ]
    }
   ],
   "source": [
    "seeds = [262950628, 239670711, 149827706, 790779946, 110053353]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_few_reduce, ds_reduce]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ManualReducedDataset + TwoStageReducedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test_f1_micro: 0.6204 ± 0.0358\n",
      "Average test_f1_macro: 0.5874 ± 0.0419\n",
      "Average test_f1_class_0: 0.6621 ± 0.0218\n",
      "Average test_f1_class_1: 0.5667 ± 0.0875\n",
      "Average test_f1_class_2: 0.6460 ± 0.0824\n",
      "Average test_f1_class_3: 0.4775 ± 0.0202\n",
      "Average test_f1_class_4: 0.5846 ± 0.0359\n"
     ]
    }
   ],
   "source": [
    "seeds = [802453211, 158255423, 704616756, 802301731, 241810196]\n",
    "all_results = []\n",
    "for seed in seeds:\n",
    "    results = run_experiment(seed, concatenate_datasets([ds_two, ds_reduce]))\n",
    "    all_results.append(results)\n",
    "clear_output(wait=False)\n",
    "metrics = [\"test_f1_micro\", \"test_f1_macro\"] + [f\"test_f1_class_{i}\" for i in range(5)]\n",
    "avg_results = {}\n",
    "for metric in metrics:\n",
    "    scores = [r[metric] for r in [x.metrics for x in all_results]]\n",
    "    avg_results[metric] = {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "for metric in metrics:\n",
    "    print(f\"Average {metric}: {avg_results[metric]['mean']:.4f} ± {avg_results[metric]['std']:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used for Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoBERTaEntity were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RoBERTaEntity were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65979053891a41f5a1350b39efe78169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb03309cc6b4de2a23d30e7008b13cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1119701862335205, 'eval_f1_micro': 0.5956937799043063, 'eval_f1_macro': 0.49572818003252783, 'eval_f1_class_0': 0.6826923076923077, 'eval_f1_class_1': 0.4155844155844156, 'eval_f1_class_2': 0.6044444444444445, 'eval_f1_class_3': 0.391304347826087, 'eval_f1_class_4': 0.38461538461538464, 'eval_runtime': 0.3599, 'eval_samples_per_second': 1161.418, 'eval_steps_per_second': 75.02, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86639b3bab84952b40e5e7eb89447f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.988167941570282, 'eval_f1_micro': 0.638755980861244, 'eval_f1_macro': 0.559051808858369, 'eval_f1_class_0': 0.7025641025641025, 'eval_f1_class_1': 0.4523809523809524, 'eval_f1_class_2': 0.6747967479674797, 'eval_f1_class_3': 0.4827586206896552, 'eval_f1_class_4': 0.4827586206896552, 'eval_runtime': 0.3604, 'eval_samples_per_second': 1159.678, 'eval_steps_per_second': 74.907, 'epoch': 2.0}\n",
      "{'loss': 0.7515, 'grad_norm': 9.953536033630371, 'learning_rate': 1.6049382716049385e-05, 'epoch': 2.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03330f7dd0274c889c0f43b2a9b1aeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2224868535995483, 'eval_f1_micro': 0.6291866028708134, 'eval_f1_macro': 0.5326521675667786, 'eval_f1_class_0': 0.717948717948718, 'eval_f1_class_1': 0.3870967741935484, 'eval_f1_class_2': 0.6334841628959276, 'eval_f1_class_3': 0.40860215053763443, 'eval_f1_class_4': 0.5161290322580645, 'eval_runtime': 0.3773, 'eval_samples_per_second': 1107.739, 'eval_steps_per_second': 71.552, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3418a8bad5544e50b13a1293f13c8f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1359838247299194, 'eval_f1_micro': 0.6674641148325359, 'eval_f1_macro': 0.5743029095754962, 'eval_f1_class_0': 0.712468193384224, 'eval_f1_class_1': 0.45901639344262296, 'eval_f1_class_2': 0.7368421052631579, 'eval_f1_class_3': 0.4470588235294118, 'eval_f1_class_4': 0.5161290322580645, 'eval_runtime': 0.4269, 'eval_samples_per_second': 979.169, 'eval_steps_per_second': 63.248, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a715b6ed9a448e49f083d5ed3301c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1057027578353882, 'eval_f1_micro': 0.6698564593301436, 'eval_f1_macro': 0.5767814554494064, 'eval_f1_class_0': 0.7304785894206549, 'eval_f1_class_1': 0.44776119402985076, 'eval_f1_class_2': 0.728, 'eval_f1_class_3': 0.46153846153846156, 'eval_f1_class_4': 0.5161290322580645, 'eval_runtime': 0.4, 'eval_samples_per_second': 1045.039, 'eval_steps_per_second': 67.503, 'epoch': 5.0}\n",
      "{'loss': 0.3482, 'grad_norm': 19.060897827148438, 'learning_rate': 9.876543209876543e-06, 'epoch': 5.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc94dbf9b104af4851aaf98bd7ac139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1563602685928345, 'eval_f1_micro': 0.69377990430622, 'eval_f1_macro': 0.6243627850765329, 'eval_f1_class_0': 0.764102564102564, 'eval_f1_class_1': 0.5671641791044776, 'eval_f1_class_2': 0.7142857142857143, 'eval_f1_class_3': 0.5137614678899083, 'eval_f1_class_4': 0.5625, 'eval_runtime': 0.4017, 'eval_samples_per_second': 1040.621, 'eval_steps_per_second': 67.217, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb05b6b33d344d4802e4d7bd483737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2259196043014526, 'eval_f1_micro': 0.6507177033492823, 'eval_f1_macro': 0.5799449155613539, 'eval_f1_class_0': 0.7513227513227513, 'eval_f1_class_1': 0.5172413793103449, 'eval_f1_class_2': 0.6484018264840182, 'eval_f1_class_3': 0.4827586206896552, 'eval_f1_class_4': 0.5, 'eval_runtime': 0.3595, 'eval_samples_per_second': 1162.624, 'eval_steps_per_second': 75.098, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c185ba86bc247fbb7e2159dbc32fcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2206114530563354, 'eval_f1_micro': 0.7033492822966507, 'eval_f1_macro': 0.6214245192148005, 'eval_f1_class_0': 0.7647058823529411, 'eval_f1_class_1': 0.5714285714285714, 'eval_f1_class_2': 0.7509881422924901, 'eval_f1_class_3': 0.52, 'eval_f1_class_4': 0.5, 'eval_runtime': 0.3764, 'eval_samples_per_second': 1110.382, 'eval_steps_per_second': 71.723, 'epoch': 8.0}\n",
      "{'loss': 0.2113, 'grad_norm': 11.433853149414062, 'learning_rate': 3.7037037037037037e-06, 'epoch': 8.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd5cfebb9034a4fa6d733423c8e20d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2487915754318237, 'eval_f1_micro': 0.6961722488038278, 'eval_f1_macro': 0.6303399887770154, 'eval_f1_class_0': 0.7506702412868632, 'eval_f1_class_1': 0.5714285714285714, 'eval_f1_class_2': 0.7419354838709677, 'eval_f1_class_3': 0.5321100917431193, 'eval_f1_class_4': 0.5555555555555556, 'eval_runtime': 0.3899, 'eval_samples_per_second': 1072.079, 'eval_steps_per_second': 69.249, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1e08ec25b54f4586efbeb604c3fa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2816280126571655, 'eval_f1_micro': 0.7105263157894737, 'eval_f1_macro': 0.6421365927969038, 'eval_f1_class_0': 0.7643979057591623, 'eval_f1_class_1': 0.5671641791044776, 'eval_f1_class_2': 0.75, 'eval_f1_class_3': 0.5576923076923077, 'eval_f1_class_4': 0.5714285714285714, 'eval_runtime': 0.3665, 'eval_samples_per_second': 1140.59, 'eval_steps_per_second': 73.674, 'epoch': 10.0}\n",
      "{'train_runtime': 98.4021, 'train_samples_per_second': 292.677, 'train_steps_per_second': 18.292, 'train_loss': 0.38877283520168726, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f109e3d45c4a3ba28dfaab6fbda8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 3.4418604 , -0.866702  , -1.2586852 ,  0.16438948, -0.4466757 ],\n",
       "       [ 3.4636855 , -1.3378413 , -0.99504375,  0.08696233, -0.14384496],\n",
       "       [ 3.9783578 , -1.8114694 , -1.2508426 , -0.06303007,  0.23558894],\n",
       "       ...,\n",
       "       [-0.02859818, -0.6438564 ,  4.405754  , -0.86341256, -2.1857705 ],\n",
       "       [ 1.509085  , -1.2781395 ,  2.6367295 ,  0.13552351, -1.7677331 ],\n",
       "       [-0.23224574,  3.0701632 ,  1.209892  , -1.4276651 , -1.9107246 ]],\n",
       "      dtype=float32), label_ids=array([3, 0, 4, 2, 2, 2, 1, 3, 2, 1, 4, 4, 0, 2, 2, 2, 1, 4, 0, 0, 3, 3,\n",
       "       3, 2, 0, 0, 1, 2, 4, 0, 1, 4, 2, 3, 1, 2, 2, 0, 1, 1, 0, 0, 4, 0,\n",
       "       0, 2, 1, 3, 2, 0, 1, 1, 0, 3, 0, 1, 0, 0, 1, 3, 2, 2, 2, 0, 2, 0,\n",
       "       0, 4, 3, 1, 2, 1, 2, 0, 2, 2, 2, 0, 0, 3, 0, 1, 1, 2, 3, 0, 0, 0,\n",
       "       0, 1, 0, 3, 2, 2, 1, 4, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 0, 2,\n",
       "       0, 3, 3, 2, 1, 2, 0, 0, 2, 0, 4, 2, 2, 0, 0, 0, 0, 0, 2, 1, 4, 2,\n",
       "       0, 4, 3, 3, 1, 2, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 4,\n",
       "       0, 0, 0, 2, 4, 2, 1, 3, 0, 2, 1, 3, 1, 2, 2, 3, 2, 0, 0, 2, 4, 1,\n",
       "       0, 1, 0, 2, 1, 3, 0, 3, 0, 2, 0, 0, 1, 1, 0, 4, 3, 2, 1, 2, 1, 3,\n",
       "       0, 2, 1, 0, 2, 0, 2, 2, 3, 1, 0, 0, 1, 1, 0, 2, 1, 3, 1, 3, 2, 2,\n",
       "       2, 0, 2, 2, 2, 0, 4, 4, 0, 3, 2, 2, 1, 4, 0, 2, 2, 2, 1, 0, 1, 0,\n",
       "       1, 3, 0, 3, 0, 0, 0, 2, 1, 2, 3, 4, 0, 3, 0, 3, 0, 0, 0, 0, 3, 2,\n",
       "       2, 3, 3, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 2,\n",
       "       0, 2, 0, 0, 2, 2, 2, 2, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 2, 0, 0, 2, 2, 2, 3, 2, 2, 4, 1, 2, 3, 3, 3, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 3, 3, 1, 0, 2, 4, 3, 0, 2, 0, 0, 2, 2, 2, 3,\n",
       "       0, 0, 2, 2, 2, 2, 1, 3, 0, 0, 0, 3, 0, 1, 1, 3, 3, 3, 0, 3, 3, 2,\n",
       "       2, 0, 2, 4, 2, 0, 0, 0, 1, 1, 2, 4, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0,\n",
       "       0, 0, 2, 0, 0, 0, 2, 2, 3, 2, 2, 1, 2, 1, 2, 0, 4, 1, 1, 2, 2, 2,\n",
       "       1, 2, 1, 2, 4, 2, 3, 3, 1, 4, 0, 4, 0, 2, 3, 4, 0, 2, 1, 0, 0, 0,\n",
       "       2, 3, 0, 0, 1, 1, 0, 2, 2, 0, 0, 2, 3, 0, 1, 0, 2, 1, 4, 0, 0, 0,\n",
       "       4, 0, 0, 3, 3, 0, 4, 0, 0, 4, 4, 0, 2, 0, 2, 2, 0, 3, 2, 2, 0, 1,\n",
       "       0, 1, 1, 0, 2, 2, 2, 0, 1, 2, 1, 3, 4, 0, 3, 2, 3, 2, 1, 2, 3, 2,\n",
       "       3, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 2, 3, 0, 3, 2, 0,\n",
       "       0, 3, 2, 0, 0, 2, 3, 0, 0, 3, 0, 2, 2, 2, 1, 4, 1, 3, 2, 2, 1, 0,\n",
       "       0, 0, 0, 4, 4, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0, 3, 3, 0,\n",
       "       3, 4, 2, 1, 1, 2, 2, 2, 3, 0, 3, 1, 1, 1, 1, 1, 2, 4, 3, 2, 1, 3,\n",
       "       0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 1, 3, 4, 4, 3, 0, 3, 1, 3,\n",
       "       2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 2, 4, 0, 0, 0, 0, 0, 2, 2, 4, 2,\n",
       "       3, 0, 0, 2, 2, 2, 0, 2, 0, 4, 0, 2, 3, 2, 3, 4, 2, 0, 0, 0, 4, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 2, 3, 2, 1, 3, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 3, 3, 2, 2, 1, 2, 0, 0, 1, 3,\n",
       "       4, 3, 4, 0, 1, 0, 3, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2,\n",
       "       0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 3, 2, 1],\n",
       "      dtype=int64), metrics={'test_loss': 0.9645255208015442, 'test_f1_micro': 0.6523489932885906, 'test_f1_macro': 0.5673258251968054, 'test_f1_class_0': 0.702865761689291, 'test_f1_class_1': 0.6255924170616114, 'test_f1_class_2': 0.7233009708737864, 'test_f1_class_3': 0.3404255319148936, 'test_f1_class_4': 0.4444444444444444, 'test_runtime': 0.5657, 'test_samples_per_second': 1317.021, 'test_steps_per_second': 83.087})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(42, concatenate_datasets([ds_few_reduce, ds_reduce]), \"../../CaseStudyModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "base_dir = \"../../CaseStudyModel\"\n",
    "subdirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "subdir = subdirs[0]\n",
    "subdir_path = os.path.join(base_dir, subdir)\n",
    "for item in os.listdir(subdir_path):\n",
    "    s = os.path.join(subdir_path, item)\n",
    "    d = os.path.join(base_dir, item)\n",
    "    if os.path.isdir(s):\n",
    "        shutil.move(s, d)\n",
    "    else:\n",
    "        shutil.move(s, d)\n",
    "os.rmdir(subdir_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
